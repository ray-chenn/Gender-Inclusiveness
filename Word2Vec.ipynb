{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "701b8b8a-5bf0-4c57-901b-32d6f73d27b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "from scipy.spatial.distance import cosine\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0948688b-a7e3-439c-a605-35d9dcbeeb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stopwords.txt') as f:\n",
    "    stopwords = f.read().replace('\\n',' ').split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d6395154-c707-404b-b688-74b77c9b55cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "history of ice hockey, notable events and people in the development of ice hockey since its creation during the 19th century. Known simply as hockey throughout North America (despite the confusion this creates with the less-prominent field hockey), it is unique among popular team sports in that it is played on top of a sheet of ice. As such, it first gained popularity in the colder climates of Canada and the northern United States, particularly in the northeast and around the Great Lakes, where iced-over lakes could serve as playing surfaces during the winter. Canada, in particular, has taken hockey to heart and made it the country’s official winter sport, becoming home to the vast majority of the game’s best players until the late 20th century. The country also provided ice hockey its most prestigious honour, the Stanley Cup, which was first awarded in 1892–93 and has, since 1926, gone to the champion of the National Hockey League (NHL), the sport’s premier level of play.\n"
     ]
    }
   ],
   "source": [
    "with open('training_text.txt', encoding='utf-8') as f:\n",
    "    text = f.read().replace('\\n','')\n",
    "    print(text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = ''.join([t for t in text if t not in list('0123456789')])\n",
    "    text = text.replace('”', '').replace('“', '').replace('’', '').lower().split()\n",
    "\n",
    "text = [w for w in text if w not in stopwords][:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7f8882cf-439c-4999-917e-18f5bef6bd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 3\n",
    "NUM_NEGATIVE_SAMPLES = 3\n",
    "\n",
    "data = []\n",
    "\n",
    "#iterate over all words\n",
    "for idx,center_word in enumerate(text[WINDOW_SIZE-1:-WINDOW_SIZE]):\n",
    "    \n",
    "    #iterate over the context words around the center word\n",
    "    context_words = [context_word for context_word in text[idx:idx+2*WINDOW_SIZE-1] if context_word != center_word]\n",
    "    for context_word in context_words:\n",
    "        \n",
    "        #get words NOT in the current context as negative examples\n",
    "        data.append([center_word, context_word, 1])\n",
    "        negative_samples = np.random.choice([w for w in text[WINDOW_SIZE-1:-WINDOW_SIZE] if w != center_word and w not in context_words], NUM_NEGATIVE_SAMPLES)\n",
    "        \n",
    "        for negative_samp in negative_samples:\n",
    "            \n",
    "            #add a training row\n",
    "            data.append([center_word, negative_samp, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a565db41-6260-4049-9052-d1df80312a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['center_word', 'context_word', 'label'], data=data)\n",
    "words = np.intersect1d(df.context_word, df.center_word)\n",
    "df = df[(df.center_word.isin(words)) & (df.context_word.isin(words))].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9647b3b0-1057-4258-9fda-d3792ca8d596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(v, scale=1):\n",
    "    return 1 / (1 + np.exp(-scale*v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7ca43bfd-07ab-4564-8d85-4ec0b70e8bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_embeddings(df, main_embeddings, context_embeddings, learning_rate, debug=False):\n",
    "    \n",
    "    #get differences between main embeddings and corresponding context embeddings\n",
    "    main_embeddings_center = main_embeddings.loc[df.center_word].values\n",
    "    context_embeddings_context = context_embeddings.loc[df.context_word].values\n",
    "    diffs = context_embeddings_context - main_embeddings_center\n",
    "    \n",
    "    #get similarities, scores, and errors between main embeddings and corresponding context embeddings\n",
    "    dot_prods = np.sum(main_embeddings_center * context_embeddings_context, axis=1)\n",
    "    scores = sigmoid(dot_prods)\n",
    "    errors = (df.label - scores).values.reshape(-1,1)\n",
    "    \n",
    "    #calculate updates\n",
    "    updates = diffs*errors*learning_rate\n",
    "    updates_df = pd.DataFrame(data=updates)\n",
    "    updates_df['center_word'] = df.center_word\n",
    "    updates_df['context_word'] = df.context_word\n",
    "    updates_df_center = updates_df.groupby('center_word').sum()\n",
    "    updates_df_context = updates_df.groupby('context_word').sum()\n",
    "    \n",
    "    if debug:\n",
    "        plot_words(debug)\n",
    "    \n",
    "    #apply updates\n",
    "    main_embeddings += updates_df_center.loc[main_embeddings.index]\n",
    "    context_embeddings -= updates_df_context.loc[context_embeddings.index]\n",
    "    \n",
    "    #normalize embeddings\n",
    "    main_embeddings = normalize_data(main_embeddings)\n",
    "    context_embeddings = normalize_data(context_embeddings)\n",
    "    \n",
    "    #return the updated embeddings\n",
    "    return main_embeddings, context_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c52aeef6-3da6-4625-8bcf-0db93e1bfd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(data):\n",
    "    row_norms = np.sqrt((data.values**2).sum(axis=1)).reshape(-1,1)\n",
    "    return data.divide(row_norms, axis='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c915be1b-1105-435c-8682-da9698d8419b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_words(debug):\n",
    "    plt.figure(figsize=(8,4))\n",
    "    \n",
    "    plt.subplot(1,2,1)\n",
    "    lim_main_first = main_embeddings.loc[[debug[0]]]\n",
    "    lim_main_second = main_embeddings.loc[[debug[1]]]\n",
    "    p1 = plt.scatter(lim_main_first[0], lim_main_first[1], color='r')\n",
    "    plt.arrow(0,0,float(lim_main_first[0]), float(lim_main_first[1]), head_width=0.01, length_includes_head=True)\n",
    "    for idx,row in lim_main_first.iterrows():\n",
    "        plt.text(row[0], row[1], str(idx))\n",
    "    p2 = plt.scatter(lim_main_second[0], lim_main_second[1], color='r')\n",
    "    plt.arrow(0,0,float(lim_main_second[0]), float(lim_main_second[1]), head_width=0.01, length_includes_head=True)\n",
    "    for idx,row in lim_main_second.iterrows():\n",
    "        plt.text(row[0], row[1], str(idx))\n",
    "    sim = 1 - cosine(main_embeddings.loc[debug[0]], main_embeddings.loc[debug[1]])\n",
    "    plt.title('Sim = %s'%round(sim,4), fontsize=20)\n",
    "    plt.axvline(0, color='k', linestyle='--', alpha=0.5)\n",
    "    plt.axhline(0, color='k', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    t = np.arange(0, 3.14*2+0.1, 0.1)\n",
    "    plt.plot(np.cos(t), np.sin(t), linewidth=1, color='k', alpha=0.5, linestyle='--')\n",
    "    \n",
    "    ###################################\n",
    "    plt.subplot(1,2,2)\n",
    "    lim_main = main_embeddings.loc[[debug[0]]]\n",
    "    lim_context = context_embeddings.loc[[debug[1]]]\n",
    "    p1 = plt.scatter(lim_main[0], lim_main[1], color='r')\n",
    "    plt.arrow(0,0,float(lim_main[0]), float(lim_main[1]), head_width=0.01, length_includes_head=True)\n",
    "    for idx,row in lim_main.iterrows():\n",
    "        plt.text(row[0], row[1], str(idx))\n",
    "    p2 = plt.scatter(lim_context[0], lim_context[1], color='b')\n",
    "    plt.arrow(0,0,float(lim_context[0]), float(lim_context[1]), head_width=0.01, length_includes_head=True)\n",
    "    for idx,row in lim_context.iterrows():\n",
    "        plt.text(row[0], row[1], str(idx))\n",
    "    sim = 1 - cosine(main_embeddings.loc[debug[0]], context_embeddings.loc[debug[1]])\n",
    "    plt.title('Sim = %s'%round(sim,4), fontsize=20)\n",
    "    plt.axvline(0, color='k', linestyle='--', alpha=0.5)\n",
    "    plt.axhline(0, color='k', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    plt.plot(np.cos(t), np.sin(t), linewidth=1, color='k', alpha=0.5, linestyle='--')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9badb4f3-acb7-4714-979b-651fdaa91613",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 2\n",
    "\n",
    "main_embeddings = np.random.normal(0,0.1,(len(words), EMBEDDING_SIZE))\n",
    "row_norms = np.sqrt((main_embeddings**2).sum(axis=1)).reshape(-1,1)\n",
    "main_embeddings = main_embeddings / row_norms\n",
    "\n",
    "context_embeddings = np.random.normal(0,0.1,(len(words), EMBEDDING_SIZE))\n",
    "row_norms = np.sqrt((context_embeddings**2).sum(axis=1)).reshape(-1,1)\n",
    "context_embeddings = context_embeddings / row_norms\n",
    "\n",
    "main_embeddings = pd.DataFrame(data=main_embeddings, index=words)\n",
    "context_embeddings = pd.DataFrame(data=context_embeddings, index=words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6c5561dd-b1d6-42e2-bf2a-7aa7d985be2f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'rules'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'rules'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 53\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m compare \u001b[38;5;129;01min\u001b[39;00m words:\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compare \u001b[38;5;241m!=\u001b[39m word:\n\u001b[1;32m---> 53\u001b[0m             sim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m cosine(\u001b[43mmain_embeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mword\u001b[49m\u001b[43m]\u001b[49m, main_embeddings\u001b[38;5;241m.\u001b[39mloc[compare])\n\u001b[0;32m     54\u001b[0m             L\u001b[38;5;241m.\u001b[39mappend((word,compare,sim))\n\u001b[0;32m     55\u001b[0m             extra\u001b[38;5;241m.\u001b[39mappend(compare)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexing.py:1191\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1189\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[0;32m   1190\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001b[1;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexing.py:1431\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1429\u001b[0m \u001b[38;5;66;03m# fall thru to straight lookup\u001b[39;00m\n\u001b[0;32m   1430\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_key(key, axis)\n\u001b[1;32m-> 1431\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_label\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexing.py:1381\u001b[0m, in \u001b[0;36m_LocIndexer._get_label\u001b[1;34m(self, label, axis)\u001b[0m\n\u001b[0;32m   1379\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_label\u001b[39m(\u001b[38;5;28mself\u001b[39m, label, axis: AxisInt):\n\u001b[0;32m   1380\u001b[0m     \u001b[38;5;66;03m# GH#5567 this will fail if the label is not present in the axis.\u001b[39;00m\n\u001b[1;32m-> 1381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\generic.py:4301\u001b[0m, in \u001b[0;36mNDFrame.xs\u001b[1;34m(self, key, axis, level, drop_level)\u001b[0m\n\u001b[0;32m   4299\u001b[0m             new_index \u001b[38;5;241m=\u001b[39m index[loc]\n\u001b[0;32m   4300\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4301\u001b[0m     loc \u001b[38;5;241m=\u001b[39m \u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(loc, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m   4304\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m loc\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mbool_:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'rules'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "L = []\n",
    "vFirst = True\n",
    "check = []\n",
    "extra = []\n",
    "wordCount = 0\n",
    "cutOff = 0\n",
    "\n",
    "with open(\"training_text.txt\", \"r\") as text:\n",
    "    stopWords = open('stopwords.txt', 'r')\n",
    "    for line in text:\n",
    "        for word in line.split():\n",
    "            if word not in stopWords:\n",
    "                wordCount = wordCount + 1;\n",
    "\n",
    "cutOff = math.ceil(wordCount / 20)\n",
    "\n",
    "with open(\"masculine.txt\", \"r\") as masculine:\n",
    "    for line in masculine:\n",
    "        for word in line.split():\n",
    "            L.clear()\n",
    "            for w1 in words:\n",
    "                for w2 in words:\n",
    "                    if w1 != w2:\n",
    "                        sim = 1 - cosine(main_embeddings.loc[w1], main_embeddings.loc[w2])\n",
    "                        L.append((w1,w2,sim))\n",
    "            for entry in sorted([item for item in L if item[0] == word], key=lambda t: -t[2])[:cutOff]:\n",
    "                with open(\"masculine.txt\", \"a\") as men:\n",
    "                    men.write(\"\\n\" + str(entry))\n",
    "\n",
    "with open(\"feminine.txt\", \"r\") as feminine:\n",
    "    for line in feminine:\n",
    "        for word in line.split():\n",
    "            L.clear()\n",
    "            women = open(\"feminine.txt\", \"a\")\n",
    "            for w1 in words:\n",
    "                for w2 in words:\n",
    "                    if w1 != w2:\n",
    "                        sim = 1 - cosine(main_embeddings.loc[w1], main_embeddings.loc[w2])\n",
    "                        L.append((w1,w2,sim))\n",
    "            for entry in sorted([item for item in L if item[0] == word], key=lambda t: -t[2])[:cutOff]:\n",
    "                women.write(\"\\n\" + str(entry))\n",
    "\n",
    "with open(\"neutral.txt\", \"r\") as neutral:\n",
    "    extra.clear()\n",
    "    for line in neutral:\n",
    "        for word in line.split():\n",
    "            L.clear()\n",
    "            neutral = open(\"neutral.txt\", \"a\")\n",
    "            for compare in words:\n",
    "                if compare != word:\n",
    "                        sim = 1 - cosine(main_embeddings.loc[word], main_embeddings.loc[compare])\n",
    "                        L.append((word,compare,sim))\n",
    "                        extra.append(compare)\n",
    "            for entry in sorted([item for item in L if item[0] == word], key=lambda t: -t[2])[:cutOff]:\n",
    "                check.append(str(entry)[len(word) + 2:])\n",
    "\n",
    "with open('verify.txt', 'a') as verify:\n",
    "    for w1 in extra:\n",
    "        for w2 in check:\n",
    "            if w1 in w2:\n",
    "                if vFirst:\n",
    "                    verify.write(w1)\n",
    "                    vFirst = False\n",
    "                else:\n",
    "                    verify.write(\"\\n\" + w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03daa003-9716-48ad-a438-efd68bbfc956",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a89bc2-f05e-43b4-bdb2-4d9543870612",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
