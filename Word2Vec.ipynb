{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "701b8b8a-5bf0-4c57-901b-32d6f73d27b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "from scipy.spatial.distance import cosine\n",
    "import matplotlib.pyplot as plt\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "0948688b-a7e3-439c-a605-35d9dcbeeb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stopwords.txt') as f:\n",
    "    stopwords = f.read().replace('\\n',' ').split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "d6395154-c707-404b-b688-74b77c9b55cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyze the following text for words similar to masculine and feminine : The study of gender and ethnic stereotypes is an importanttopic across many disciplines. Language analysis is a standardtool used to discover, understand, and demonstrate such stereotypes(1–5). Previous literature broadly establishes that languageboth reflects and perpetuates cultural stereotypes. However,such studies primarily leverage human surveys (6–16), dictionaryand qualitative analysis (17), or in-depth knowledge of differentlanguages (18). These methods often require time-consumingand expensive manual analysis and may not easily scale acrosstypes of stereotypes, time periods, and languages. In this paper,we propose using word embeddings, a commonly used tool innatural language processing (NLP) and machine learning, as aframework to measure, quantify, and compare beliefs over time.As a specific case study, we apply this tool to study the temporaldynamics of gender and ethnic stereotypes in the 20th and 21stcenturies in the United States.In word-embedding models, each word in a given language isassigned to a high-dimensional vector such that the geometry ofthe vectors captures semantic relations between the words—e.g.,vectors being closer together has been shown to correspond tomore similar words (19). These models are typically trained automaticallyon large corpora of text, such as collections of GoogleNews articles or Wikipedia, and are known to capture relationshipsnot found through simple co-occurrence analysis. For example,the vector for France is close to vectors for Austria and Italy,and the vector forXBoxis close to that of PlayStation (19). Beyondnearby neighbors, embeddings can also capture more global relationshipsbetween words. The difference between London andEngland—obtained by simply subtracting these two vectors—isparallel to the vector difference between Paris and France. Thispattern allows embeddings to capture analogy relationships, suchas London to England is as Paris to France.\n"
     ]
    }
   ],
   "source": [
    "with open('training_text.txt', encoding='utf-8') as f:\n",
    "    text = f.read().replace('\\n','')\n",
    "    print(text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = ''.join([t for t in text if t not in list('0123456789')])\n",
    "    text = text.replace('”', '').replace('“', '').replace('’', '').lower().split()\n",
    "\n",
    "text = [w for w in text if w not in stopwords][:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "7f8882cf-439c-4999-917e-18f5bef6bd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 3\n",
    "NUM_NEGATIVE_SAMPLES = 3\n",
    "\n",
    "data = []\n",
    "\n",
    "#iterate over all words\n",
    "for idx,center_word in enumerate(text[WINDOW_SIZE-1:-WINDOW_SIZE]):\n",
    "    \n",
    "    #iterate over the context words around the center word\n",
    "    context_words = [context_word for context_word in text[idx:idx+2*WINDOW_SIZE-1] if context_word != center_word]\n",
    "    for context_word in context_words:\n",
    "        \n",
    "        #get words NOT in the current context as negative examples\n",
    "        data.append([center_word, context_word, 1])\n",
    "        negative_samples = np.random.choice([w for w in text[WINDOW_SIZE-1:-WINDOW_SIZE] if w != center_word and w not in context_words], NUM_NEGATIVE_SAMPLES)\n",
    "        \n",
    "        for negative_samp in negative_samples:\n",
    "            \n",
    "            #add a training row\n",
    "            data.append([center_word, negative_samp, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "a565db41-6260-4049-9052-d1df80312a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['center_word', 'context_word', 'label'], data=data)\n",
    "words = np.intersect1d(df.context_word, df.center_word)\n",
    "df = df[(df.center_word.isin(words)) & (df.context_word.isin(words))].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "9647b3b0-1057-4258-9fda-d3792ca8d596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(v, scale=1):\n",
    "    return 1 / (1 + np.exp(-scale*v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "7ca43bfd-07ab-4564-8d85-4ec0b70e8bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_embeddings(df, main_embeddings, context_embeddings, learning_rate, debug=False):\n",
    "    \n",
    "    #get differences between main embeddings and corresponding context embeddings\n",
    "    main_embeddings_center = main_embeddings.loc[df.center_word].values\n",
    "    context_embeddings_context = context_embeddings.loc[df.context_word].values\n",
    "    diffs = context_embeddings_context - main_embeddings_center\n",
    "    \n",
    "    #get similarities, scores, and errors between main embeddings and corresponding context embeddings\n",
    "    dot_prods = np.sum(main_embeddings_center * context_embeddings_context, axis=1)\n",
    "    scores = sigmoid(dot_prods)\n",
    "    errors = (df.label - scores).values.reshape(-1,1)\n",
    "    \n",
    "    #calculate updates\n",
    "    updates = diffs*errors*learning_rate\n",
    "    updates_df = pd.DataFrame(data=updates)\n",
    "    updates_df['center_word'] = df.center_word\n",
    "    updates_df['context_word'] = df.context_word\n",
    "    updates_df_center = updates_df.groupby('center_word').sum()\n",
    "    updates_df_context = updates_df.groupby('context_word').sum()\n",
    "    \n",
    "    if debug:\n",
    "        plot_words(debug)\n",
    "    \n",
    "    #apply updates\n",
    "    main_embeddings += updates_df_center.loc[main_embeddings.index]\n",
    "    context_embeddings -= updates_df_context.loc[context_embeddings.index]\n",
    "    \n",
    "    #normalize embeddings\n",
    "    main_embeddings = normalize_data(main_embeddings)\n",
    "    context_embeddings = normalize_data(context_embeddings)\n",
    "    \n",
    "    #return the updated embeddings\n",
    "    return main_embeddings, context_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "c52aeef6-3da6-4625-8bcf-0db93e1bfd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(data):\n",
    "    row_norms = np.sqrt((data.values**2).sum(axis=1)).reshape(-1,1)\n",
    "    return data.divide(row_norms, axis='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "c915be1b-1105-435c-8682-da9698d8419b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_words(debug):\n",
    "    plt.figure(figsize=(8,4))\n",
    "    \n",
    "    plt.subplot(1,2,1)\n",
    "    lim_main_first = main_embeddings.loc[[debug[0]]]\n",
    "    lim_main_second = main_embeddings.loc[[debug[1]]]\n",
    "    p1 = plt.scatter(lim_main_first[0], lim_main_first[1], color='r')\n",
    "    plt.arrow(0,0,float(lim_main_first[0]), float(lim_main_first[1]), head_width=0.01, length_includes_head=True)\n",
    "    for idx,row in lim_main_first.iterrows():\n",
    "        plt.text(row[0], row[1], str(idx))\n",
    "    p2 = plt.scatter(lim_main_second[0], lim_main_second[1], color='r')\n",
    "    plt.arrow(0,0,float(lim_main_second[0]), float(lim_main_second[1]), head_width=0.01, length_includes_head=True)\n",
    "    for idx,row in lim_main_second.iterrows():\n",
    "        plt.text(row[0], row[1], str(idx))\n",
    "    sim = 1 - cosine(main_embeddings.loc[debug[0]], main_embeddings.loc[debug[1]])\n",
    "    plt.title('Sim = %s'%round(sim,4), fontsize=20)\n",
    "    plt.axvline(0, color='k', linestyle='--', alpha=0.5)\n",
    "    plt.axhline(0, color='k', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    t = np.arange(0, 3.14*2+0.1, 0.1)\n",
    "    plt.plot(np.cos(t), np.sin(t), linewidth=1, color='k', alpha=0.5, linestyle='--')\n",
    "    \n",
    "    ###################################\n",
    "    plt.subplot(1,2,2)\n",
    "    lim_main = main_embeddings.loc[[debug[0]]]\n",
    "    lim_context = context_embeddings.loc[[debug[1]]]\n",
    "    p1 = plt.scatter(lim_main[0], lim_main[1], color='r')\n",
    "    plt.arrow(0,0,float(lim_main[0]), float(lim_main[1]), head_width=0.01, length_includes_head=True)\n",
    "    for idx,row in lim_main.iterrows():\n",
    "        plt.text(row[0], row[1], str(idx))\n",
    "    p2 = plt.scatter(lim_context[0], lim_context[1], color='b')\n",
    "    plt.arrow(0,0,float(lim_context[0]), float(lim_context[1]), head_width=0.01, length_includes_head=True)\n",
    "    for idx,row in lim_context.iterrows():\n",
    "        plt.text(row[0], row[1], str(idx))\n",
    "    sim = 1 - cosine(main_embeddings.loc[debug[0]], context_embeddings.loc[debug[1]])\n",
    "    plt.title('Sim = %s'%round(sim,4), fontsize=20)\n",
    "    plt.axvline(0, color='k', linestyle='--', alpha=0.5)\n",
    "    plt.axhline(0, color='k', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    plt.plot(np.cos(t), np.sin(t), linewidth=1, color='k', alpha=0.5, linestyle='--')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "9badb4f3-acb7-4714-979b-651fdaa91613",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 2\n",
    "\n",
    "main_embeddings = np.random.normal(0,0.1,(len(words), EMBEDDING_SIZE))\n",
    "row_norms = np.sqrt((main_embeddings**2).sum(axis=1)).reshape(-1,1)\n",
    "main_embeddings = main_embeddings / row_norms\n",
    "\n",
    "context_embeddings = np.random.normal(0,0.1,(len(words), EMBEDDING_SIZE))\n",
    "row_norms = np.sqrt((context_embeddings**2).sum(axis=1)).reshape(-1,1)\n",
    "context_embeddings = context_embeddings / row_norms\n",
    "\n",
    "main_embeddings = pd.DataFrame(data=main_embeddings, index=words)\n",
    "context_embeddings = pd.DataFrame(data=context_embeddings, index=words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "6c5561dd-b1d6-42e2-bf2a-7aa7d985be2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['across', 'acrosstypes', 'aframework', 'allows', 'analysis', 'andengland—obtained', 'apply', 'articles', 'austria', 'automaticallyon', 'beliefs', 'beyondnearby', 'broadly', 'capture', 'captures', 'case', 'close', 'closer', 'collections', 'commonly', 'cooccurrence', 'correspond', 'cultural', 'demonstrate', 'dictionaryand', 'differentlanguages', 'disciplines', 'establishes', 'expensive', 'feminine', 'found', 'gender', 'geometry', 'global', 'highdimensional', 'howeversuch', 'human', 'importanttopic', 'indepth', 'innatural', 'italyand', 'known', 'language', 'languageboth', 'large', 'learning', 'leverage', 'london', 'machine', 'manual', 'many', 'masculine', 'may', 'measure', 'models', 'neighbors', 'nlp', 'often', 'paperwe', 'paris', 'perpetuates', 'playstation', 'primarily', 'processing', 'propose', 'reflects', 'relations', 'relationships', 'relationshipsnot', 'require', 'similar', 'simply', 'specific', 'standardtool', 'stcenturies', 'stereotypes', 'stereotypes–', 'subtracting', 'suchas', 'surveys', 'temporaldynamics', 'text', 'thispattern', 'time', 'timeconsumingand', 'together', 'tomore', 'tool', 'trained', 'two', 'typically', 'understand', 'united', 'using', 'vectors—isparallel', 'wikipedia', 'wordembedding', 'words—egvectors', 'across', 'acrosstypes', 'aframework', 'allows', 'analysis', 'andengland—obtained', 'apply', 'articles', 'automaticallyon', 'beliefs', 'broadly', 'capture', 'captures', 'case', 'closer', 'collections', 'compare', 'cooccurrence', 'correspond', 'cultural', 'demonstrate', 'dictionaryand', 'difference', 'differentlanguages', 'disciplines', 'discover', 'easily', 'embeddings', 'establishes', 'ethnic', 'expensive', 'feminine', 'forxboxis', 'geometry', 'given', 'global', 'googlenews', 'highdimensional', 'human', 'importanttopic', 'isassigned', 'knowledge', 'language', 'languageboth', 'languages', 'learning', 'machine', 'manual', 'many', 'masculine', 'may', 'measure', 'methods', 'ofthe', 'paperwe', 'periods', 'perpetuates', 'previous', 'primarily', 'qualitative', 'quantify', 'reflects', 'relationships', 'relationshipsbetween', 'require', 'scale', 'semantic', 'shown', 'simply', 'standardtool', 'statesin', 'stcenturies', 'stereotypes', 'stereotypes–', 'studies', 'subtracting', 'suchas', 'time', 'timeas', 'timeconsumingand', 'together', 'tool', 'trained', 'two', 'typically', 'united', 'used', 'using', 'vectors', 'wikipedia', 'words']\n"
     ]
    }
   ],
   "source": [
    "check = []\n",
    "start = False\n",
    "\n",
    "#avoid words with numbers followed by measurment units (ie. 100y, 5d, 10cm, etc)\n",
    "for word in words:\n",
    "    sim = 1 - cosine(main_embeddings.loc['masculine'], main_embeddings.loc[word])\n",
    "    if (round(sim, 2)) >= 0.50 or (round(sim, 2)) <= -0.50:\n",
    "        check.append(word)\n",
    "\n",
    "for word in words:\n",
    "    sim = 1 - cosine(main_embeddings.loc['feminine'], main_embeddings.loc[word])\n",
    "    if (round(sim, 2)) >= 0.50 or (round(sim, 2)) <= -0.50:\n",
    "        check.append(word)\n",
    "print(check)\n",
    "def chat_with_gpt(prompt):\n",
    "                response = openai.ChatCompletion.create(\n",
    "                model = \"gpt-4-turbo\",\n",
    "                messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "                )\n",
    "                return response.choices[0].message.content.strip()\n",
    "for word in check:\n",
    "    if __name__ == \"__main__\":\n",
    "        if word != 'masculine' and word != 'feminine':\n",
    "            response = chat_with_gpt(\"Is the word \" + word + \" closer to masculinity, femininity or neutral? Give a one word answer.\")\n",
    "            if response.lower()[0] == \"m\": \n",
    "                f = open('masculine.txt', 'a')\n",
    "                f.write(word + \"\\n\")\n",
    "                f.close()\n",
    "            if response.lower()[0] == \"f\": \n",
    "                f = open('feminine.txt', 'a')\n",
    "                f.write(word + \"\\n\")\n",
    "                f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "30b690b7-2ea7-4afb-8f9f-bbc81231d1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There does not exist inclusiveness issues.\n"
     ]
    }
   ],
   "source": [
    "words = 0\n",
    "stopwords = []\n",
    "masculine = 0\n",
    "feminine = 0\n",
    "#remove double quotation marks and citations/references if any\n",
    "with open('training_text.txt') as f1, open('stopwords.txt') as f2:\n",
    "    for line2 in f2:\n",
    "        for word in line2.split():\n",
    "            stopwords.append(word)\n",
    "    for line1 in f1:\n",
    "        for word in line1.split():\n",
    "            if (word) not in stopwords:\n",
    "                words += 1\n",
    "                \n",
    "with open(\"masculine.txt\", 'r') as fp:\n",
    "    masculine = len(fp.readlines())\n",
    "\n",
    "with open(\"feminine.txt\", 'r') as fp:\n",
    "    feminine = len(fp.readlines())\n",
    "\n",
    "if (masculine >= words * 0.05):\n",
    "    if (masculine >= (masculine + feminine) * 0.75):\n",
    "        print(\"There exists inclusiveness issues.\")\n",
    "    else:\n",
    "        print(\"There does not exist inclusiveness issues.\")\n",
    "else:\n",
    "    print(\"There does not exist inclusiveness issues.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ba8623-1845-45ac-8372-7ce760dafa55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03daa003-9716-48ad-a438-efd68bbfc956",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
