{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "701b8b8a-5bf0-4c57-901b-32d6f73d27b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "from scipy.spatial.distance import cosine\n",
    "import matplotlib.pyplot as plt\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0cdb67f-86e6-4d47-bce9-7b9fb4c45d48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = 0\n",
    "stopwords = []\n",
    "#remove double quotation marks and citations/references if any\n",
    "with open('training_text.txt') as f1, open('stopwords.txt') as f2:\n",
    "    for line2 in f2:\n",
    "        for word in line2.split():\n",
    "            stopwords.append(word)\n",
    "    for line1 in f1:\n",
    "        for word in line1.split():\n",
    "            if (word) not in stopwords:\n",
    "                words += 1\n",
    "\n",
    "words - 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0948688b-a7e3-439c-a605-35d9dcbeeb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stopwords.txt') as f:\n",
    "    stopwords = f.read().replace('\\n',' ').split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6395154-c707-404b-b688-74b77c9b55cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyze the following text for words similar to masculine and feminine : Word embeddings are a powerful machine-learning frameworkthat represents each English word by a vector. The geometricrelationship between these vectors captures meaningful semanticrelationships between the corresponding words. In this paper, wedevelop a framework to demonstrate how the temporal dynamicsof the embedding helps to quantify changes in stereotypes andattitudes toward women and ethnic minorities in the 20th and21st centuries in the United States. We integrate word embeddingstrained on 100 years of text data with the US Census to showthat changes in the embedding track closely with demographicand occupation shifts over time. The embedding captures societalshifts—e.g., the women’s movement in the 1960s and Asian immigrationinto the United States—and also illuminates how specificadjectives and occupations became more closely associated withcertain populations over time. Our framework for temporal analysisof word embedding opens up a fruitful intersection betweenmachine learning and quantitative social science.\n"
     ]
    }
   ],
   "source": [
    "with open('training_text.txt', encoding='utf-8') as f:\n",
    "    text = f.read().replace('\\n','')\n",
    "    print(text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = ''.join([t for t in text if t not in list('0123456789')])\n",
    "    text = text.replace('”', '').replace('“', '').replace('’', '').lower().split()\n",
    "\n",
    "text = [w for w in text if w not in stopwords][:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f8882cf-439c-4999-917e-18f5bef6bd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 3\n",
    "NUM_NEGATIVE_SAMPLES = 3\n",
    "\n",
    "data = []\n",
    "\n",
    "#iterate over all words\n",
    "for idx,center_word in enumerate(text[WINDOW_SIZE-1:-WINDOW_SIZE]):\n",
    "    \n",
    "    #iterate over the context words around the center word\n",
    "    context_words = [context_word for context_word in text[idx:idx+2*WINDOW_SIZE-1] if context_word != center_word]\n",
    "    for context_word in context_words:\n",
    "        \n",
    "        #get words NOT in the current context as negative examples\n",
    "        data.append([center_word, context_word, 1])\n",
    "        negative_samples = np.random.choice([w for w in text[WINDOW_SIZE-1:-WINDOW_SIZE] if w != center_word and w not in context_words], NUM_NEGATIVE_SAMPLES)\n",
    "        \n",
    "        for negative_samp in negative_samples:\n",
    "            \n",
    "            #add a training row\n",
    "            data.append([center_word, negative_samp, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a565db41-6260-4049-9052-d1df80312a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['center_word', 'context_word', 'label'], data=data)\n",
    "words = np.intersect1d(df.context_word, df.center_word)\n",
    "df = df[(df.center_word.isin(words)) & (df.context_word.isin(words))].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9647b3b0-1057-4258-9fda-d3792ca8d596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(v, scale=1):\n",
    "    return 1 / (1 + np.exp(-scale*v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7ca43bfd-07ab-4564-8d85-4ec0b70e8bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_embeddings(df, main_embeddings, context_embeddings, learning_rate, debug=False):\n",
    "    \n",
    "    #get differences between main embeddings and corresponding context embeddings\n",
    "    main_embeddings_center = main_embeddings.loc[df.center_word].values\n",
    "    context_embeddings_context = context_embeddings.loc[df.context_word].values\n",
    "    diffs = context_embeddings_context - main_embeddings_center\n",
    "    \n",
    "    #get similarities, scores, and errors between main embeddings and corresponding context embeddings\n",
    "    dot_prods = np.sum(main_embeddings_center * context_embeddings_context, axis=1)\n",
    "    scores = sigmoid(dot_prods)\n",
    "    errors = (df.label - scores).values.reshape(-1,1)\n",
    "    \n",
    "    #calculate updates\n",
    "    updates = diffs*errors*learning_rate\n",
    "    updates_df = pd.DataFrame(data=updates)\n",
    "    updates_df['center_word'] = df.center_word\n",
    "    updates_df['context_word'] = df.context_word\n",
    "    updates_df_center = updates_df.groupby('center_word').sum()\n",
    "    updates_df_context = updates_df.groupby('context_word').sum()\n",
    "    \n",
    "    if debug:\n",
    "        plot_words(debug)\n",
    "    \n",
    "    #apply updates\n",
    "    main_embeddings += updates_df_center.loc[main_embeddings.index]\n",
    "    context_embeddings -= updates_df_context.loc[context_embeddings.index]\n",
    "    \n",
    "    #normalize embeddings\n",
    "    main_embeddings = normalize_data(main_embeddings)\n",
    "    context_embeddings = normalize_data(context_embeddings)\n",
    "    \n",
    "    #return the updated embeddings\n",
    "    return main_embeddings, context_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c52aeef6-3da6-4625-8bcf-0db93e1bfd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(data):\n",
    "    row_norms = np.sqrt((data.values**2).sum(axis=1)).reshape(-1,1)\n",
    "    return data.divide(row_norms, axis='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c915be1b-1105-435c-8682-da9698d8419b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_words(debug):\n",
    "    plt.figure(figsize=(8,4))\n",
    "    \n",
    "    plt.subplot(1,2,1)\n",
    "    lim_main_first = main_embeddings.loc[[debug[0]]]\n",
    "    lim_main_second = main_embeddings.loc[[debug[1]]]\n",
    "    p1 = plt.scatter(lim_main_first[0], lim_main_first[1], color='r')\n",
    "    plt.arrow(0,0,float(lim_main_first[0]), float(lim_main_first[1]), head_width=0.01, length_includes_head=True)\n",
    "    for idx,row in lim_main_first.iterrows():\n",
    "        plt.text(row[0], row[1], str(idx))\n",
    "    p2 = plt.scatter(lim_main_second[0], lim_main_second[1], color='r')\n",
    "    plt.arrow(0,0,float(lim_main_second[0]), float(lim_main_second[1]), head_width=0.01, length_includes_head=True)\n",
    "    for idx,row in lim_main_second.iterrows():\n",
    "        plt.text(row[0], row[1], str(idx))\n",
    "    sim = 1 - cosine(main_embeddings.loc[debug[0]], main_embeddings.loc[debug[1]])\n",
    "    plt.title('Sim = %s'%round(sim,4), fontsize=20)\n",
    "    plt.axvline(0, color='k', linestyle='--', alpha=0.5)\n",
    "    plt.axhline(0, color='k', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    t = np.arange(0, 3.14*2+0.1, 0.1)\n",
    "    plt.plot(np.cos(t), np.sin(t), linewidth=1, color='k', alpha=0.5, linestyle='--')\n",
    "    \n",
    "    ###################################\n",
    "    plt.subplot(1,2,2)\n",
    "    lim_main = main_embeddings.loc[[debug[0]]]\n",
    "    lim_context = context_embeddings.loc[[debug[1]]]\n",
    "    p1 = plt.scatter(lim_main[0], lim_main[1], color='r')\n",
    "    plt.arrow(0,0,float(lim_main[0]), float(lim_main[1]), head_width=0.01, length_includes_head=True)\n",
    "    for idx,row in lim_main.iterrows():\n",
    "        plt.text(row[0], row[1], str(idx))\n",
    "    p2 = plt.scatter(lim_context[0], lim_context[1], color='b')\n",
    "    plt.arrow(0,0,float(lim_context[0]), float(lim_context[1]), head_width=0.01, length_includes_head=True)\n",
    "    for idx,row in lim_context.iterrows():\n",
    "        plt.text(row[0], row[1], str(idx))\n",
    "    sim = 1 - cosine(main_embeddings.loc[debug[0]], context_embeddings.loc[debug[1]])\n",
    "    plt.title('Sim = %s'%round(sim,4), fontsize=20)\n",
    "    plt.axvline(0, color='k', linestyle='--', alpha=0.5)\n",
    "    plt.axhline(0, color='k', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    plt.plot(np.cos(t), np.sin(t), linewidth=1, color='k', alpha=0.5, linestyle='--')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9badb4f3-acb7-4714-979b-651fdaa91613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run until all pairs shown have similarity scores of 1.0 or stop when one gender has over x words\n",
    "EMBEDDING_SIZE = 2\n",
    "\n",
    "main_embeddings = np.random.normal(0,0.1,(len(words), EMBEDDING_SIZE))\n",
    "row_norms = np.sqrt((main_embeddings**2).sum(axis=1)).reshape(-1,1)\n",
    "main_embeddings = main_embeddings / row_norms\n",
    "\n",
    "context_embeddings = np.random.normal(0,0.1,(len(words), EMBEDDING_SIZE))\n",
    "row_norms = np.sqrt((context_embeddings**2).sum(axis=1)).reshape(-1,1)\n",
    "context_embeddings = context_embeddings / row_norms\n",
    "\n",
    "main_embeddings = pd.DataFrame(data=main_embeddings, index=words)\n",
    "context_embeddings = pd.DataFrame(data=context_embeddings, index=words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6c5561dd-b1d6-42e2-bf2a-7aa7d985be2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": 
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b690b7-2ea7-4afb-8f9f-bbc81231d1a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ba8623-1845-45ac-8372-7ce760dafa55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03daa003-9716-48ad-a438-efd68bbfc956",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
